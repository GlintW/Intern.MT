使用一个类似 y = Wx + b 的线性函数, 模拟训练数据.

1. 构造一批模拟的一元二次函数的数据, 也即是生成大量x和对应的y
2. 定义神经网络的输入
3. 定义神经层(输入层, 隐藏层, 输出层)
    在tensorflow中, 神经层常使用 y = Wx + b 的Weights(权重)和biases(偏移)和激励函数作为参数.
4. 学习

定义神经层, 通过第一个输入层, 将输入传给隐藏的神经层, 经过激励函数处理成输出, 再作为下一个隐藏层的输入, 反复多次, 最终得到输出.

张量 tensor
"张量"即是多维数组, 用"阶"(D)表示张量的维数
0D -> 0阶张量 -> 标量 scalar -> s = 1
1D -> 1阶张量 -> 矢量 vector -> v = [1, 2, 3]
2D -> 2阶张量 -> 矩阵 matrix -> m = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
nD -> n阶张量 -> 张量 tensor -> t = [[[ ... ]]]

数据类型
tf.float32  tf.int32 ...

神经网络(neural network, NN)实现过程
1. 准备数据, 提取特征
2. 数据输入到NN, 先搭建graph, 然后执行session(前向传播)
3. 大量输入得到大量输出, 把每次的输出与标准答案的差异再次传给NN, 调整NN的参数直到模型达到要求(反向传播)
4. 得到优化好参数的模型(训练好的模型), 对新数据进行模型预测
准备 -> 前传 -> 反传 -> 迭代

神经元 neuron
在NN neural network中, 一个节点被称作一个neuron

计算图 graph
搭建神经网络的计算过程, 只搭建, 不运算

会话 session
执行计算图中的节点运算, 执行global_variables_initializer()这个赋值相当于执行了输入节点.
